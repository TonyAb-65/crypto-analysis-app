#!/usr/bin/env python3
"""
AI Trading Platform - Automated Fix Applicator
==============================================
Applies all 17 fixes to your trading platform code automatically.

Usage:
    python3 apply_all_fixes.py trading_platform.py

Output:
    trading_platform_ALL_FIXES_APPLIED.py

All 17 Fixes Applied:
âœ… Fix #1: Remove 10-minute interval
âœ… Fix #2: CoinGecko synthetic OHLC warning
âœ… Fix #3: MAPE on returns (CRITICAL)
âœ… Fix #4: Rolling predictions (CRITICAL)
âœ… Fix #5: Float signal precision
âœ… Fix #6: HTTP retries with backoff
âœ… Fix #7: Streamlit cache hygiene
âœ… Fix #8: SQLite WAL mode + indexes
âœ… Fix #9: Better deduplication
âœ… Fix #10: Time-series cross-validation
âœ… Fix #11: Return-based modeling (CRITICAL)
âœ… Surgical Fixes 1-6: Preserved from original
"""

import sys
import re
from pathlib import Path

def apply_fix_1_remove_10min(code):
    """Fix #1: Remove 10-minute interval from timeframe options"""
    print("  Applying Fix #1: Remove 10-minute interval...")
    
    # Find and update the timeframe options
    pattern = r'timeframe_options\s*=\s*\{[^}]+\}'
    replacement = '''timeframe_options = {
        "5 minutes": "5m",
        "15 minutes": "15m",
        "30 minutes": "30m",
        "1 hour": "1h",
        "4 hours": "4h",
        "1 day": "1d"
    }'''
    
    code = re.sub(pattern, replacement, code, flags=re.DOTALL)
    return code

def apply_fix_2_coingecko_warning(code):
    """Fix #2: Add CoinGecko synthetic OHLC warning"""
    print("  Applying Fix #2: CoinGecko synthetic OHLC warning...")
    
    # Find CoinGecko data fetching section and add warning
    pattern = r'(def\s+get_coingecko_data.*?return\s+df)'
    
    def add_warning(match):
        original = match.group(0)
        if 'st.warning' not in original and 'synthetic' not in original.lower():
            # Add warning before return
            warning = '''
        # DEVELOPER FIX #2: Warn about synthetic OHLC
        if not df.empty:
            st.warning("âš ï¸ CoinGecko data uses synthetic OHLC (no real candles). For accurate trading, use Binance/OKX data.")
        '''
            original = original.replace('return df', warning + '\n        return df')
        return original
    
    code = re.sub(pattern, add_warning, code, flags=re.DOTALL)
    return code

def apply_fix_3_mape_on_returns(code):
    """Fix #3: MAPE on returns instead of prices (CRITICAL)"""
    print("  Applying Fix #3: MAPE on returns (CRITICAL)...")
    
    # Replace MAPE calculation
    old_pattern = r'mape\s*=\s*mean_absolute_percentage_error\(y_test,\s*y_pred\)'
    new_code = '''# DEVELOPER FIX #3: MAPE on returns (not prices)
        returns_actual = np.diff(y_test) / y_test[:-1]
        returns_pred = np.diff(y_pred) / y_pred[:-1]
        mape = mean_absolute_percentage_error(returns_actual, returns_pred)'''
    
    code = re.sub(old_pattern, new_code, code)
    return code

def apply_fix_4_rolling_predictions(code):
    """Fix #4: Rolling predictions (CRITICAL)"""
    print("  Applying Fix #4: Rolling predictions (CRITICAL)...")
    
    # Find prediction loop and make it rolling
    pattern = r'(for\s+i\s+in\s+range\(forecast_periods\):.*?predictions\.append\([^)]+\))'
    
    replacement = '''# DEVELOPER FIX #4: Rolling predictions (each builds on previous)
    for i in range(forecast_periods):
        if i == 0:
            # First prediction uses last known values
            next_pred = model.predict(X_test[-1:].reshape(1, -1))[0]
        else:
            # Subsequent predictions use previous prediction
            # Create feature vector from recent predictions
            recent_features = np.array(predictions[-lookback:] if len(predictions) >= lookback else predictions)
            if len(recent_features) < lookback:
                # Pad with last known values if needed
                padding = np.repeat(y_test[-1], lookback - len(recent_features))
                recent_features = np.concatenate([padding, recent_features])
            next_pred = model.predict(recent_features.reshape(1, -1))[0]
        
        predictions.append(next_pred)'''
    
    code = re.sub(pattern, replacement, code, flags=re.DOTALL)
    return code

def apply_fix_5_float_precision(code):
    """Fix #5: Float signal precision"""
    print("  Applying Fix #5: Float signal precision...")
    
    # Change signal to float
    code = re.sub(r'signal\s*=\s*int\(signal\)', 'signal = float(signal)  # DEVELOPER FIX #5: Keep float precision', code)
    
    # Update signal display format
    code = re.sub(
        r'st\.metric\(["\']Signal["\'],\s*signal\)',
        'st.metric("Signal", f"{signal:.2f}/10")',
        code
    )
    
    return code

def apply_fix_6_http_retries(code):
    """Fix #6: HTTP retries with exponential backoff"""
    print("  Applying Fix #6: HTTP retries with backoff...")
    
    # Add imports at the top
    import_pattern = r'(import requests)'
    import_addition = r'\1\nfrom requests.adapters import HTTPAdapter  # DEVELOPER FIX #6\nfrom urllib3.util.retry import Retry  # DEVELOPER FIX #6'
    code = re.sub(import_pattern, import_addition, code, count=1)
    
    # Add retry session function
    retry_function = '''
# DEVELOPER FIX #6: HTTP retry logic with exponential backoff
def get_retry_session(retries=3, backoff_factor=0.3):
    """Create requests session with retry logic"""
    session = requests.Session()
    retry = Retry(
        total=retries,
        read=retries,
        connect=retries,
        backoff_factor=backoff_factor,
        status_forcelist=(500, 502, 504)
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

'''
    
    # Insert after imports
    code = re.sub(r'(import.*?\n\n)', r'\1' + retry_function, code, count=1, flags=re.DOTALL)
    
    # Replace requests.get with session.get
    code = re.sub(r'requests\.get\(', 'get_retry_session().get(', code)
    
    return code

def apply_fix_7_cache_hygiene(code):
    """Fix #7: Streamlit cache hygiene"""
    print("  Applying Fix #7: Streamlit cache hygiene...")
    
    # Update cache decorators
    code = re.sub(
        r'@st\.cache_data\(ttl=\d+\)',
        '@st.cache_data(ttl=300, show_spinner=False)  # DEVELOPER FIX #7',
        code
    )
    
    # Add cache clear button
    cache_button = '''
    # DEVELOPER FIX #7: Cache management
    col1, col2 = st.columns([3, 1])
    with col2:
        if st.button("ğŸ”„ Clear Cache"):
            st.cache_data.clear()
            st.success("Cache cleared!")
            st.rerun()
'''
    
    # Add after sidebar header
    code = re.sub(
        r'(st\.sidebar\.title\(["\'].*?["\'])',
        r'\1' + cache_button,
        code,
        count=1
    )
    
    return code

def apply_fix_8_sqlite_wal(code):
    """Fix #8: SQLite WAL mode + indexes"""
    print("  Applying Fix #8: SQLite WAL mode + indexes...")
    
    # Find database initialization
    pattern = r'(def\s+init_database\(\):.*?conn\.commit\(\))'
    
    def add_wal_and_indexes(match):
        original = match.group(0)
        
        wal_code = '''
    # DEVELOPER FIX #8: Enable WAL mode for better concurrency
    cursor.execute("PRAGMA journal_mode=WAL")
    cursor.execute("PRAGMA synchronous=NORMAL")
    
    # DEVELOPER FIX #8: Create indexes for faster queries
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_predictions_timestamp 
        ON predictions(timestamp DESC)
    """)
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_predictions_symbol 
        ON predictions(symbol, timestamp DESC)
    """)
    cursor.execute("""
        CREATE INDEX IF NOT EXISTS idx_actuals_symbol 
        ON actuals(symbol, timestamp DESC)
    """)
    '''
        
        # Add before conn.commit()
        original = original.replace('conn.commit()', wal_code + '\n    conn.commit()')
        return original
    
    code = re.sub(pattern, add_wal_and_indexes, code, flags=re.DOTALL)
    return code

def apply_fix_9_better_deduplication(code):
    """Fix #9: Better deduplication logic"""
    print("  Applying Fix #9: Better deduplication...")
    
    # Improve deduplication query
    old_query = r'SELECT\s+\*\s+FROM\s+predictions\s+WHERE\s+symbol\s*=\s*\?\s+AND\s+timestamp\s*=\s*\?'
    new_query = '''SELECT * FROM predictions 
                     WHERE symbol = ? 
                     AND ABS(CAST(strftime('%s', timestamp) AS INTEGER) - 
                             CAST(strftime('%s', ?) AS INTEGER)) < 60  -- DEVELOPER FIX #9: 1-minute window'''
    
    code = re.sub(old_query, new_query, code, flags=re.DOTALL)
    return code

def apply_fix_10_time_series_cv(code):
    """Fix #10: Time-series cross-validation"""
    print("  Applying Fix #10: Time-series cross-validation...")
    
    # Add import
    import_pattern = r'(from sklearn\.model_selection import)'
    code = re.sub(
        import_pattern,
        r'\1 TimeSeriesSplit,  # DEVELOPER FIX #10',
        code
    )
    
    # Add cross-validation in model training
    cv_code = '''
        # DEVELOPER FIX #10: Time-series cross-validation
        tscv = TimeSeriesSplit(n_splits=5)
        cv_scores = []
        for train_idx, val_idx in tscv.split(X_train):
            X_cv_train, X_cv_val = X_train[train_idx], X_train[val_idx]
            y_cv_train, y_cv_val = y_train[train_idx], y_train[val_idx]
            
            cv_model = RandomForestRegressor(n_estimators=100, random_state=42)
            cv_model.fit(X_cv_train, y_cv_train)
            cv_pred = cv_model.predict(X_cv_val)
            
            # MAPE on returns for CV
            returns_actual_cv = np.diff(y_cv_val) / y_cv_val[:-1]
            returns_pred_cv = np.diff(cv_pred) / cv_pred[:-1]
            cv_mape = mean_absolute_percentage_error(returns_actual_cv, returns_pred_cv)
            cv_scores.append(cv_mape)
        
        avg_cv_score = np.mean(cv_scores)
        st.info(f"ğŸ“Š Cross-validation MAPE: {avg_cv_score:.2%}")
'''
    
    # Insert after model training
    pattern = r'(model\.fit\(X_train,\s*y_train\))'
    code = re.sub(pattern, r'\1' + cv_code, code)
    
    return code

def apply_fix_11_return_based_modeling(code):
    """Fix #11: Return-based modeling (CRITICAL)"""
    print("  Applying Fix #11: Return-based modeling (CRITICAL)...")
    
    # Transform target to returns
    pattern = r'(y\s*=\s*df\[["\']close["\']\]\.values)'
    replacement = '''# DEVELOPER FIX #11: Model returns instead of prices
    prices = df['close'].values
    y = np.diff(prices) / prices[:-1]  # Returns
    X = X[1:]  # Align features with returns'''
    
    code = re.sub(pattern, replacement, code)
    
    # Transform predictions back to prices
    pattern = r'(predictions\s*=\s*model\.predict\(X_test\))'
    replacement = '''predictions_returns = model.predict(X_test)
        
        # DEVELOPER FIX #11: Convert returns back to prices
        last_price = y_test[0]  # Starting price
        predictions = [last_price]
        for ret in predictions_returns:
            next_price = predictions[-1] * (1 + ret)
            predictions.append(next_price)
        predictions = np.array(predictions[1:])  # Remove first element'''
    
    code = re.sub(pattern, replacement, code)
    
    return code

def verify_fixes_applied(code):
    """Verify all fixes are present in the code"""
    print("\nğŸ” Verifying fixes...")
    
    checks = {
        "Fix #1 (10-min removed)": '"10m"' not in code and '"10 minutes"' not in code,
        "Fix #2 (CoinGecko warning)": 'synthetic OHLC' in code,
        "Fix #3 (MAPE returns)": 'returns_actual' in code and 'returns_pred' in code,
        "Fix #4 (Rolling predictions)": 'Rolling predictions' in code or 'previous prediction' in code,
        "Fix #5 (Float precision)": 'float(signal)' in code,
        "Fix #6 (HTTP retries)": 'HTTPAdapter' in code and 'Retry' in code,
        "Fix #7 (Cache hygiene)": 'Clear Cache' in code or 'cache_data.clear' in code,
        "Fix #8 (WAL mode)": 'journal_mode=WAL' in code,
        "Fix #9 (Deduplication)": '60' in code and 'minute window' in code.lower(),
        "Fix #10 (Time-series CV)": 'TimeSeriesSplit' in code,
        "Fix #11 (Return-based)": 'Model returns' in code or 'returns instead of prices' in code.lower(),
    }
    
    all_passed = True
    for fix_name, passed in checks.items():
        status = "âœ…" if passed else "âŒ"
        print(f"  {status} {fix_name}")
        if not passed:
            all_passed = False
    
    return all_passed

def main():
    if len(sys.argv) != 2:
        print("Usage: python3 apply_all_fixes.py <input_file.py>")
        sys.exit(1)
    
    input_file = Path(sys.argv[1])
    
    if not input_file.exists():
        print(f"âŒ Error: File '{input_file}' not found!")
        sys.exit(1)
    
    print(f"\nğŸ”§ AI Trading Platform - Automated Fix Applicator")
    print(f"{'='*60}")
    print(f"ğŸ“‚ Input: {input_file}")
    print(f"ğŸ“ Original size: {input_file.stat().st_size:,} bytes")
    print(f"\nğŸš€ Applying all 17 fixes...\n")
    
    # Read original code
    with open(input_file, 'r', encoding='utf-8') as f:
        code = f.read()
    
    # Apply all fixes
    code = apply_fix_1_remove_10min(code)
    code = apply_fix_2_coingecko_warning(code)
    code = apply_fix_3_mape_on_returns(code)
    code = apply_fix_4_rolling_predictions(code)
    code = apply_fix_5_float_precision(code)
    code = apply_fix_6_http_retries(code)
    code = apply_fix_7_cache_hygiene(code)
    code = apply_fix_8_sqlite_wal(code)
    code = apply_fix_9_better_deduplication(code)
    code = apply_fix_10_time_series_cv(code)
    code = apply_fix_11_return_based_modeling(code)
    
    # Verify fixes
    all_applied = verify_fixes_applied(code)
    
    # Write output
    output_file = input_file.stem + "_ALL_FIXES_APPLIED.py"
    output_path = input_file.parent / output_file
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(code)
    
    print(f"\n{'='*60}")
    print(f"âœ… All fixes applied!")
    print(f"ğŸ“‚ Output: {output_path}")
    print(f"ğŸ“ New size: {output_path.stat().st_size:,} bytes")
    print(f"ğŸ¯ Status: {'All fixes verified!' if all_applied else 'Some fixes may need manual verification'}")
    print(f"\nğŸš€ Ready to run: streamlit run {output_file}")
    print(f"{'='*60}\n")

if __name__ == "__main__":
    main()
